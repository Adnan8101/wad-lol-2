Database
ID Name Age Gender Income Purchased (isme age ka data kuch missing rakho)(10 entries minimum)

pip install pandas seaborn matplotlib scikit-learn	

1. Data Visualization, Handling Missing Values, Outliers, and Duplicates
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the simple dummy dataset
df = pd.read_csv('simple_dummy_database.csv')

# Handle missing values (fill numeric columns with mean)
df.fillna(df.mean(numeric_only=True), inplace=True)

# Remove duplicates
df.drop_duplicates(inplace=True)

# Visualize age distribution
plt.figure(figsize=(8, 5))
sns.histplot(df['Age'], bins=10, kde=True)
plt.title("Age Distribution")
plt.show()




2. Apply Linear Regression and Evaluate Performance
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load the simple dummy dataset
df = pd.read_csv('simple_dummy_database.csv')

# Define feature and target variable
X = df[['Income']]
y = df['Purchased']

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions and evaluate
y_pred = model.predict(X_test)
print("Mean Squared Error (MSE):", mean_squared_error(y_test, y_pred))
print("R-squared Score:", r2_score(y_test, y_pred))




3. Apply Naïve Bayes on Categorical Data
import pandas as pd
from sklearn.naive_bayes import CategoricalNB
from sklearn.preprocessing import LabelEncoder

# Load the simple dummy dataset
df = pd.read_csv('simple_dummy_database.csv')

# Define features and target
X = df[['Gender']]
y = df['Purchased']

# Encode categorical data
le = LabelEncoder()
X = X.apply(le.fit_transform)

# Train Naïve Bayes model
model = CategoricalNB()
model.fit(X, y)

# Make a prediction for a new sample (Female)
prediction = model.predict([[0]])
print("Prediction for Female:", prediction)




4. Evaluate Feature Engineering Techniques
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# Load the simple dummy dataset
df = pd.read_csv('simple_dummy_database.csv')

# Prepare features and target
X = df[['Age', 'Income']].fillna(0)
y = df['Purchased']

# Train Random Forest model
model = RandomForestClassifier(random_state=42)
model.fit(X, y)

# Display feature importance
print("Feature Importances:", model.feature_importances_)




5. DBSCAN on a Synthetic Dataset
from sklearn.datasets import make_blobs
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# Generate synthetic data
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)

# Apply DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X)

# Visualize clusters
plt.figure(figsize=(8, 5))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.title("DBSCAN Clustering")
plt.show()




6. K-Means Clustering with Different K Values
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Generate synthetic data
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)

# Apply KMeans with different K values
for k in range(2, 6):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)

    # Plot clusters
    plt.figure(figsize=(8, 5))
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
    plt.title(f"K-Means Clustering with K = {k}")
    plt.show()




7. Decision Tree with Different Feature Subsets
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the simple dummy dataset
df = pd.read_csv('simple_dummy_database.csv')

# Train and evaluate Decision Tree with different feature subsets
for features in [['Age'], ['Age', 'Income']]:
    X = df[features].fillna(0)
    y = df['Purchased']

    # Train the model
    model = DecisionTreeClassifier(random_state=42)
    model.fit(X, y)

    # Make predictions and evaluate
    y_pred = model.predict(X)
    accuracy = accuracy_score(y, y_pred)
    print(f"Features: {features}, Accuracy: {accuracy:.2f}")
